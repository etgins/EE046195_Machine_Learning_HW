{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Copy of assignment_6_ex.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etgins/Machine-Learning-HW/blob/main/Copy_of_assignment_6_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L061cJckcO7"
      },
      "source": [
        "# Assignment 6 - Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M81GKHXckcPH"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "Remember to:\n",
        "\n",
        "1. Make your own copy of the notebook by pressing the \"Copy to drive\" button.\n",
        "2. Expend all cells by pressing **Ctrl+[**\n",
        "\n",
        "### Your IDs\n",
        "\n",
        "✍️ Fill in your IDs in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMiGuuuqkcPJ"
      },
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Fill in your IDs (as a string)\n",
        "student1_id = '...'\n",
        "student2_id = '...'\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print('Hello ' + student1_id + ' & ' + student2_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mmJOUsCkcPL"
      },
      "source": [
        "### Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DFfBapHkcPL"
      },
      "source": [
        "import numpy as np  # Numerical package (mainly multi-dimensional arrays and linear algebra)\n",
        "import pandas as pd  # A package for working with data frames\n",
        "import matplotlib.pyplot as plt  # A plotting package\n",
        "import tqdm.notebook as tqdm  # A progress bar package\n",
        "\n",
        "## Setup matplotlib to output figures into the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "## Set some default values of the the matplotlib plots\n",
        "plt.rcParams['figure.figsize'] = (6.0, 6.0)  # Set default plot's sizes\n",
        "plt.rcParams['figure.dpi'] = 120  # Set default plot's dpi (increase fonts' size)\n",
        "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IzjQ9CHkcPM"
      },
      "source": [
        "## Labeled Voices Dataset\n",
        "\n",
        "In this assignment we will use the same dataset as we have used in tutorial 11 (SVM) which contains the properties of 3168 voice samples along with the gender of the speaker.This dataset was collected and processed by Kory Becker and was published on [her website](http://www.primaryobjects.com/2016/06/22/identifying-the-gender-of-a-voice-using-machine-learning/)\n",
        "\n",
        "According to the project's web page, the 3168 voice samples and their label were collected from the following resources:\n",
        "\n",
        "- [The Harvard-Haskins Database of Regularly-Timed Speech](http://www.nsi.edu/~ani/download.html)\n",
        "- Telecommunications & Signal Processing Laboratory (TSP) Speech Database at McGill University\n",
        "- [VoxForge Speech Corpus](http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/8kHz_16bit/)\n",
        "- [Festvox CMU_ARCTIC Speech Database at Carnegie Mellon University](http://festvox.org/cmu_arctic/)\n",
        "\n",
        "Each voice track was then processed using a tool called [WarbleR](https://cran.r-project.org/web/packages/warbleR/warbleR.pdf) in order to extract 20 numerical features for each track.\n",
        "\n",
        "The dataset can be found [here](https://technion046195.netlify.app/datasets/voice.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKE7stAmkcPO"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKGFqm2SkcPP"
      },
      "source": [
        "data_file = 'https://technion046195.netlify.app/datasets/voice.csv'\n",
        "\n",
        "## Loading the data\n",
        "dataset = pd.read_csv(data_file)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVO-xYOvkcPP"
      },
      "source": [
        "## The Data Fields and Types\n",
        "\n",
        "The list and descriptions of the data fields as copied from the project's website:\n",
        "\n",
        "- **meanfreq**: mean frequency (in kHz)\n",
        "- **sd**: standard deviation of frequency\n",
        "- **median**: median frequency (in kHz)\n",
        "- **Q25**: first quantile (in kHz)\n",
        "- **Q75**: third quantile (in kHz)\n",
        "- **IQR**: interquantile range (in kHz)\n",
        "- **skew**: skewness (see note in specprop description)\n",
        "- **kurt**: kurtosis (see note in specprop description)\n",
        "- **sp.ent**: spectral entropy\n",
        "- **sfm**: spectral flatness\n",
        "- **mode**: mode frequency\n",
        "- **centroid**: frequency centroid (see specprop)\n",
        "- **meanfun**: average of fundamental frequency measured across acoustic signal\n",
        "- **minfun**: minimum fundamental frequency measured across acoustic signal\n",
        "- **maxfun**: maximum fundamental frequency measured across acoustic signal\n",
        "- **meandom**: average of dominant frequency measured across acoustic signal\n",
        "- **mindom**: minimum of dominant frequency measured across acoustic signal\n",
        "- **maxdom**: maximum of dominant frequency measured across acoustic signal\n",
        "- **dfrange**: range of dominant frequency measured across acoustic signal\n",
        "- **modindx**: modulation index. Calculated as the accumulated absolute difference between\n",
        "\n",
        "- **label**: The label of each track: male/female"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDxMGMDpkcPQ"
      },
      "source": [
        "We shall store all the relevant fields in a list named \"x_fields\" and the field of the label as \"y_field\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz55lTO3kcPR"
      },
      "source": [
        "y_field = 'label'\n",
        "\n",
        "x_fields = ['meanfreq', 'sd', 'median', 'Q25', 'Q75', 'IQR', 'skew', 'kurt',\n",
        "            'sp.ent', 'sfm', 'mode', 'centroid', 'meanfun', 'minfun', 'maxfun',\n",
        "            'meandom', 'mindom', 'maxdom', 'dfrange', 'modindx']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK0qZHeikcPS"
      },
      "source": [
        "## Train-Validation-Test split\n",
        "\n",
        "✍️ Complete the code below to split the data into 60% train 20% validation set set and 20% test set similar to the last assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMgiwtf-kcPS"
      },
      "source": [
        "n_samples = len(dataset)  # The total number of samples in the dataset\n",
        "\n",
        "## Generate a random generator with a fixed seed\n",
        "rand_gen = np.random.RandomState(1)\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "## Generating a shuffled vector of indices\n",
        "...\n",
        "\n",
        "## Split the indices into 60% train / 20% validation / 20% test\n",
        "...\n",
        "train_indices = ...\n",
        "val_indices = ...\n",
        "test_indices = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "## Extract the sub datasets from the full dataset using the calculated indices\n",
        "train_set = dataset.iloc[train_indices]\n",
        "val_set = dataset.iloc[val_indices]\n",
        "test_set = dataset.iloc[test_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm10jm3UkcPT"
      },
      "source": [
        "## AdaBoost\n",
        "\n",
        "We will implement the AdaBoost algorithm using stumps (depth 1 trees). \n",
        "\n",
        "### An efficient calculation of the weighted misclassification rate.\n",
        "\n",
        "In every step of AdaBoost we need to go over all the possibles stumps, going all the possible thresholds for each of the fields. It turns out that there is a quick way to calculate the weighted misclassification rate for all the thresholds for some fields at once. To simplify the derivation we will assume that each value of $\\text{x}_k$ in the dataset is unique):\n",
        "\n",
        "1. We will choose a specific field of $\\mathbf{x}$: $\\text{x}_k$.\n",
        "2. We will sort the samples according to $x_k^{(i)}$.\n",
        "3. We would like to calculate the weighted misclassification rate for stumps in the following form for each sample in the dataset $\\boldsymbol{x}^{(m)}$:\n",
        "\n",
        "$$\n",
        "h(\\boldsymbol{x})=2I\\{x_k>x_k^{(m)}\\}-1\n",
        "=\\begin{cases}\n",
        "1 & x_k>x_k^{(m)} \\\\\n",
        "-1 & \\text{else} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "4. After sorting the samples we can write the weighted misclassification rate for the above stump in the following way:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sum_{i=1}^N w_iI\\{y^{(i)}\\neq h(\\boldsymbol{x}^{(i)})\\}\n",
        "&=\\sum_{i=1}^N w_iI\\{y^{(i)}\\neq(2I\\{x_k^{(i)}>x_k^{(m)}\\}-1)\\}\\\\\n",
        "&=\\sum_{i=1}^N w_i\\left( I\\{y^{(i)}=1\\}I\\{x_k^{(i)}\\leq x_k^{(m)}\\}\n",
        "                        +I\\{y^{(i)}=-1\\}I\\{x_k^{(i)}>x_k^{(m)}\\}\\right)\\\\\n",
        "&=\\sum_{i=1}^N w_i\\left( \\tfrac{1}{2}(1+y^{(i)})I\\{x_k^{(i)}\\leq x_k^{(m)}\\}\n",
        "                        +\\tfrac{1}{2}(1-y^{(i)})I\\{x_k^{(i)}>x_k^{(m)}\\}\\right)\\\\\n",
        "&=\\tfrac{1}{2}\\sum_{i=1}^N w_i\\left( I\\{x_k^{(i)}\\leq x_k^{(m)}\\}\n",
        "                                    +I\\{x_k^{(i)}>x_k^{(m)}\\}\n",
        "                                    +y^{(i)}\\left( I\\{x_k^{(i)}\\leq x_k^{(m)}\\}\n",
        "                                                  -I\\{x_k^{(i)}>x_k^{(m)}\\}\\right)\n",
        "                              \\right)\\\\\n",
        "&=\\tfrac{1}{2}\\sum_{i=1}^N w_i\\left(1+y^{(i)}\\left(2I\\{x_k^{(i)}\\leq x_k^{(m)}\\}-1\\right)\\right)\\\\\n",
        "&=\\tfrac{1}{2}\\left( \\sum_{i=1}^N w_i\n",
        "                    -\\sum_{i=1}^N w_i y^{(i)}\n",
        "                    +2\\sum_{i=1}^N w_i y^{(i)}I\\{x_k^{(i)}\\leq x_k^{(m)}\\}\n",
        "              \\right)\\\\\n",
        "&=\\tfrac{1}{2}\\left(1-\\sum_{i=1}^N w_i y^{(i)}+2\\sum_{i=1}^m w_i y^{(i)}\\right)\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "In the above formula the only dependency on $m$ is in the last summation. We can quickly calculate the values for all values of $m\\in[1,N]$ using the function [numpy.cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html).\n",
        "\n",
        "The method *find_threshold* in the code bellow implements this calculation.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "✍️ Complete the code below to implement a the AdaBoost calculation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfjhRK3UkcPb"
      },
      "source": [
        "def stump_predict(dataset, field, threshold, sign):\n",
        "    \"\"\"\n",
        "    An auxiliary function to calculate a stump classification, i.e.:\n",
        "      h(x) = 2I{x > threshold} - 1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset: Pandas DataFrame\n",
        "        The dataset on which to apply the classification.\n",
        "    field: string\n",
        "        The field on which to apply the threshold\n",
        "    threshold: float\n",
        "        The threshold\n",
        "    sign: integer: -1 or 1\n",
        "        An integer indication if the thresholf should be:\n",
        "        - x > threshold (for sign = 1)\n",
        "        - x < threshold (for sign = -1)        \n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    y_hat: ndarray\n",
        "        The vector of predictions\n",
        "    \"\"\"\n",
        "\n",
        "    if sign > 0:\n",
        "        y_hat = (dataset[field].values > threshold) * 2 - 1\n",
        "    else:\n",
        "        y_hat = (dataset[field].values < threshold) * 2 - 1\n",
        "    return y_hat\n",
        "    \n",
        "class AdaBoost:\n",
        "    def __init__(self, train_set, x_fields, y_field):\n",
        "        \"\"\"\n",
        "        Initizaling the class fields.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        train_set: Pandas DataFrame\n",
        "            The train set.\n",
        "        x_fields: list of strings\n",
        "            The fields of x in the dataset.\n",
        "        y_field: string\n",
        "            The field containting the label y.\n",
        "        \"\"\"\n",
        "\n",
        "        self.train_set = train_set\n",
        "        self.x_fields = x_fields\n",
        "        self.y_field = y_field\n",
        "        \n",
        "        self.y_train = (self.train_set[self.y_field] == 'male').values * 2 - 1  # Extract the y vector as a -1,1 labels\n",
        "        self.thresholds = []  # This field will store the stumps thresholds\n",
        "        self.signs = []  # This field will store if the threshold should be > or <.\n",
        "        self.fields = []  # This field will store the stump according to which the threshold will be applied.\n",
        "        self.alphas = []  # This fields will store AbaBoost's coefficients.\n",
        "        self.weights = np.ones(len(train_set)) / len(train_set)  # Initialized AdaBoost's wieghts.\n",
        "\n",
        "    def find_threshold(self, field, sign, weights):\n",
        "        \"\"\"\n",
        "        Given a field and a weight vector, this method finds the threshold which minimizes \n",
        "        the weighted misclassification rate.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        field: string\n",
        "            The field of x which is to be used for the thresholding.\n",
        "        sign: integer: -1 or 1\n",
        "            An integer indication if the thresholf should be:\n",
        "            - x > threshold (for sign = 1)\n",
        "            - x < threshold (for sign = -1)        \n",
        "        weights: ndarray\n",
        "            The 1D array of the AdaBoost weights of the samples.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        threshold: float\n",
        "            The optimal threshold.\n",
        "        sign: integer: -1 or 1\n",
        "            An integer indication if the thresholf should be:\n",
        "            - x > threshold (for sign = 1)\n",
        "            - x < threshold (for sign = -1)        \n",
        "        \"\"\"\n",
        "\n",
        "        x = self.train_set[field].values  # Extract the relevant field.\n",
        "        indices = np.argsort(x)  # Find the indices which sorts the samples.\n",
        "        if sign < 0:\n",
        "            indices = indices[::-1]  # if sign = -1 reverse indices so that x will be in descending order.\n",
        "\n",
        "        sorted_x = x[indices]  # Sort x\n",
        "        signed_sorted_weights = weights[indices] * self.y_train[indices]  # Calculate w_i * y_i.\n",
        "        weighted_misclass_list = 0.5 * (1 - signed_sorted_weights.sum() + 2 * np.cumsum(signed_sorted_weights))  # Calculate the weighted misclassification rate for each threshold.\n",
        "        \n",
        "        index = np.argmin(weighted_misclass_list)  # Find the index which minimizes the weighted miscalssifiaction rate\n",
        "        threshold = sorted_x[index]\n",
        "        \n",
        "        return threshold\n",
        "        \n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Preform a single AdaBoost step claculating the following parameters for the current step:\n",
        "        - field: The field on which the stump will apply the thershold.\n",
        "        - threshold: The stumps threshold.\n",
        "        - alpha: The AdaBoost's coefficients for the current stump.\n",
        "        \"\"\"\n",
        "\n",
        "        weights = self.weights.copy()  # Make a copy of the weights vector.\n",
        "        \n",
        "        ## Loop over all possible thresholds\n",
        "        ## ---------------------------------\n",
        "        ## Loop over all fields and over the sign, check the weighted misclassification rate, select the\n",
        "        ## one with the minimal and store it's parameters (field, threshold and sign)\n",
        "        best_field = None\n",
        "        best_threshold = None\n",
        "        best_sign = None\n",
        "        best_weighted_misclass = np.inf\n",
        "\n",
        "        for field in self.x_fields:\n",
        "            for sign in [-1, 1]:\n",
        "                threshold = self.find_threshold(field, sign, weights)\n",
        "                \n",
        "                ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "                ## Calcualte the weighted misclassification rate: sum_over_i(w_i * I{y_i == h(x_i)})\n",
        "                ## - Use the function \"stump_predict\" to calculate h(x_i)\n",
        "                ## - Calcualte the expression on \"self.train_set\".\n",
        "                ## - Use \"self.y_train\" for y_i\n",
        "                weighted_misclass = ...\n",
        "                ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "                ## Check if the weighted misclassification rate is the best so far and store the relevant parameters\n",
        "                if weighted_misclass < best_weighted_misclass:\n",
        "                    best_field = field\n",
        "                    best_threshold = threshold\n",
        "                    best_sign = sign\n",
        "                    best_weighted_misclass = weighted_misclass\n",
        "\n",
        "        field = best_field\n",
        "        threshold = best_threshold\n",
        "        sign = best_sign\n",
        "        weighted_misclass = best_weighted_misclass\n",
        "        \n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        ## Perform the AdaBoost step\n",
        "        ## -------------------------\n",
        "        ## Store the stump parameters\n",
        "        self.fields.append(field)\n",
        "        self.thresholds.append(...\n",
        "        self.signs.append(...\n",
        "\n",
        "        ## Calcualte alpha for the current stump\n",
        "        alpha = ...\n",
        "        self.alphas.append(alpha)\n",
        "\n",
        "        ## Update the weights\n",
        "        ## Calcualte the unnormalized weights\n",
        "        weights *= ...\n",
        "        ## Normalized the weights\n",
        "        weights /= ...\n",
        "        self.weights = weights\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "        \n",
        "    def calc_objective(self):\n",
        "        \"\"\"\n",
        "        An auxilizary method for calculating AdaBoost's objective.\n",
        "        This function is not needed for running and using AdaBoost. It is used here only for ploting purpose.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        objective: float\n",
        "            AdaBoost's objective.\n",
        "        \"\"\"\n",
        "\n",
        "        objective = 0\n",
        "        for field, threshold, sign, alpha in zip(self.fields, self.thresholds, self.signs, self.alphas):\n",
        "            objective += alpha * stump_predict(self.train_set, field, threshold, sign) * self.y_train\n",
        "        objective = np.exp(-objective).mean()\n",
        "        return objective\n",
        "        \n",
        "    def predict(self, dataset):\n",
        "        \"\"\"\n",
        "        This method calculates the prediction using the set of stumps and alphas for a given dataset.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset: Pandas DataFrame\n",
        "            The dataset on which to apply the classification.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        y_hat: ndarray\n",
        "            The vector of predictions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_y_hat = pd.Series(0, index=dataset.index)  # Initializing the linear combination of predictions\n",
        "        \n",
        "        ## Loop over the stumps to calculate the linear combination of predictions sum_t(alpha_t * h_t(x))\n",
        "        ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "        for field, threshold, sign, alpha in zip(self.fields, self.thresholds, self.signs, self.alphas):\n",
        "            raw_y_hat += ...\n",
        "        ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "        y_hat = np.sign(raw_y_hat).astype(int)  # Apply the sign function\n",
        "        return y_hat\n",
        "\n",
        "adaboost = AdaBoost(train_set, x_fields, y_field)  ## Create an AdaBoost object\n",
        "adaboost.step()  # Preform a step\n",
        "## Print the selected stump in the first step:\n",
        "stump_sign = '>' if (adaboost.signs[0] > 0) else '<'\n",
        "print(f'The selected stump: {adaboost.alphas[0]:.4f} * ({adaboost.fields[0]} {stump_sign} {adaboost.thresholds[0]:.2f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GrZeCXkcPm"
      },
      "source": [
        "You should get some thing like: \"1.5245 * (meanfun < 0.14)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLRvUiCGkcPp"
      },
      "source": [
        "The following function calculates the misclassification rate for an AdaBoost instance on a given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1LIC3RdkcPp"
      },
      "source": [
        "def calc_score(adaboost, dataset):\n",
        "    \"\"\"\n",
        "    Calculates the misclassification rate of a predictor on a given dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adaboost: AdaBoost\n",
        "        An AdaBoost object\n",
        "    dataset: Pandas DataFrame\n",
        "        The dataset on which to apply the classification.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    score: scalar\n",
        "        The evaluated score on the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    ## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "    y_hat = ...\n",
        "    score = ...\n",
        "    ## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "    return score\n",
        "\n",
        "print(f'The score on the train set is: {calc_score(adaboost, train_set):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SYmt9gkkcPq"
      },
      "source": [
        "The score for a AdaBoost with a single step on the train set should be less then 5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCiJy5D1kcPr"
      },
      "source": [
        "### Run AdaBoost\n",
        "\n",
        "The following code runs AdaBoost for 500 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS2jiHMIkcPs"
      },
      "source": [
        "adaboost = AdaBoost(train_set, x_fields, y_field)\n",
        "\n",
        "steps_list = []\n",
        "train_score_list = []\n",
        "val_score_list = []\n",
        "objective_list = []\n",
        "weighted_misclass_list = []\n",
        "\n",
        "for step in tqdm.tqdm(range(1, 501)):\n",
        "    weighted_misclass = adaboost.step()\n",
        "\n",
        "    if (step % 10) == 0:\n",
        "        train_score = calc_score(adaboost, train_set)\n",
        "        val_score = calc_score(adaboost, val_set)\n",
        "\n",
        "        steps_list.append(step)\n",
        "        train_score_list.append(train_score)\n",
        "        val_score_list.append(val_score)\n",
        "        weighted_misclass_list.append(weighted_misclass)\n",
        "        objective_list.append(adaboost.calc_objective())\n",
        "    \n",
        "## Plot\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "ax.plot(steps_list, train_score_list, label='Train')\n",
        "ax.plot(steps_list, val_score_list, label='Validation')\n",
        "ax.plot(steps_list, objective_list, label='Objective')\n",
        "# ax.set_title(r'$\\eta={' + f'{eta:g}' + r'}$')\n",
        "ax.set_ylim(0, 0.05)\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Score')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWdv-nNWkcPt"
      },
      "source": [
        "Print the first 10 stump and coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JJVFJk3kcPt"
      },
      "source": [
        "for i in range(10):\n",
        "    stump_sign = '>' if (adaboost.signs[i] < 0) else '<'\n",
        "    print(f'Step {i+1}: {adaboost.alphas[i]:.4f} * ({adaboost.fields[i]} {stump_sign} {adaboost.thresholds[i]:.2f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytBuffSbkcPt"
      },
      "source": [
        "## Final evaluation\n",
        "\n",
        "Evaluate the model using the test set\n",
        "\n",
        "✍️ Complete the code below the evaluate the misclassification rate on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyq3RasDkcPu"
      },
      "source": [
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "test_score = ...\n",
        "## %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print(f'The misclassification rate on the test is: {test_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd-okryqkcPv"
      },
      "source": [
        "You should get less then 2.5% error (take a look at the results from tutorial 11)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G3b9FISkcPw"
      },
      "source": [
        "## AdaBoost in scikit-learn\n",
        "\n",
        "The SciKit-learn package also has an implementation for [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
        "\n",
        "The code bellow uses SciKit-learn to run AdaBoost for 500 steps.\n",
        "\n",
        "✍️ Complete the code below the evaluate the misclassification rate on the test set (don't use the \"calc_score\" function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKA_y3tkcPx"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier(n_estimators=500, algorithm='SAMME')\n",
        "clf.fit(train_set[x_fields].values, train_set[y_field].values)\n",
        "\n",
        "## %%%%%%%%%%%%%%% Your code here - Begin %%%%%%%%%%%%%%%\n",
        "test_score = ...\n",
        "# %%%%%%%%%%%%%%% Your code here - End %%%%%%%%%%%%%%%%%\n",
        "\n",
        "print(test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz3E8qMlkcPx"
      },
      "source": [
        "## Submission\n",
        "\n",
        "To submit your code download it as a **ipynb** file from Colab, and upload it to the course's website (Moodle). You can download this code by selecting **Download .ipynb** from the **file** menu."
      ]
    }
  ]
}